{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHJZmNu79yo1"
   },
   "source": [
    "# **Initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CwF9-uPxqq9H",
    "ExecuteTime": {
     "end_time": "2024-04-10T16:43:57.516776Z",
     "start_time": "2024-04-10T16:43:55.019047Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "# from google.colab import drive\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from numpy import array, log\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1-noCOLGrM-"
   },
   "source": [
    "#### Mount google drive where the csv files are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ufJgt1IKGp-R",
    "outputId": "637928ad-31b2-49bc-93ed-df4c4d713503",
    "ExecuteTime": {
     "end_time": "2024-04-10T16:44:01.600254Z",
     "start_time": "2024-04-10T16:44:01.597032Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "# drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNwiOsie-Rwj"
   },
   "source": [
    "### Confirm if drive is mounted by listing the drive contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-kDJtseNLTkv",
    "ExecuteTime": {
     "end_time": "2024-04-10T16:44:04.712988Z",
     "start_time": "2024-04-10T16:44:04.709856Z"
    }
   },
   "outputs": [],
   "source": [
    "# !ls \"/content/drive/My Drive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "J8ulL16loykm",
    "ExecuteTime": {
     "end_time": "2024-04-10T16:44:05.895336Z",
     "start_time": "2024-04-10T16:44:05.893057Z"
    }
   },
   "outputs": [],
   "source": [
    "# news_data = pd.read_csv('/content/drive/My Drive/all_news_data/data.csv')\n",
    "# traffic_data = pd.read_csv('/content/drive/My Drive/all_news_data/traffic.csv')\n",
    "# domains_location_data = pd.read_csv('/content/drive/My Drive/all_news_data/domains_location.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "J8ulL16loykm",
    "ExecuteTime": {
     "end_time": "2024-04-10T16:44:13.819414Z",
     "start_time": "2024-04-10T16:44:06.538669Z"
    }
   },
   "outputs": [],
   "source": [
    "news_data = pd.read_csv('../data/data.csv')\n",
    "traffic_data = pd.read_csv('../data/traffic.csv')\n",
    "domains_location_data = pd.read_csv('../data/domains_location.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JKg5Fb43MThF",
    "outputId": "170a9835-24ef-414e-9301-78e0e83128d9",
    "ExecuteTime": {
     "end_time": "2024-04-10T16:44:15.492614Z",
     "start_time": "2024-04-10T16:44:15.486947Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['article_id', 'source_id', 'source_name', 'author', 'title',\n       'description', 'url', 'url_to_image', 'published_at', 'content',\n       'category', 'full_content'],\n      dtype='object')"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "r2ns2ieqR7eM",
    "outputId": "4ae4d56e-1b12-4351-f952-a16c0c199428",
    "ExecuteTime": {
     "end_time": "2024-04-10T16:44:19.474285Z",
     "start_time": "2024-04-10T16:44:19.464408Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   article_id source_id                   source_name  \\\n0       89541       NaN  International Business Times   \n1       89542       NaN                    Prtimes.jp   \n2       89543       NaN                      VOA News   \n3       89545       NaN            The Indian Express   \n4       89547       NaN           The Times of Israel   \n\n                                       author  \\\n0                              Paavan MATHEMA   \n1                                         NaN   \n2  webdesk@voanews.com (Agence France-Presse)   \n3                                   Editorial   \n4                                 Jacob Magid   \n\n                                               title  \\\n0  UN Chief Urges World To 'Stop The Madness' Of ...   \n1              RANDEBOOよりワンランク上の大人っぽさが漂うニットとベストが新登場。   \n2  UN Chief Urges World to 'Stop the Madness' of ...   \n3  Sikkim warning: Hydroelectricity push must be ...   \n4  200 foreigners, dual nationals cut down in Ham...   \n\n                                         description  \\\n0  UN Secretary-General Antonio Guterres urged th...   \n1  [株式会社Ainer]\\nRANDEBOO（ランデブー）では2023年7月18日(火)より公...   \n2  UN Secretary-General Antonio Guterres urged th...   \n3  Ecologists caution against the adverse effects...   \n4  France lost 35 citizens, Thailand 33, US 31, U...   \n\n                                                 url  \\\n0  https://www.ibtimes.com/un-chief-urges-world-s...   \n1  https://prtimes.jp/main/html/rd/p/000000147.00...   \n2  https://www.voanews.com/a/un-chief-urges-world...   \n3  https://indianexpress.com/article/opinion/edit...   \n4  https://www.timesofisrael.com/200-foreigners-d...   \n\n                                        url_to_image  \\\n0  https://d.ibtimes.com/en/full/4496078/nepals-g...   \n1  https://prtimes.jp/i/32220/147/ogp/d32220-147-...   \n2  https://gdb.voanews.com/01000000-0a00-0242-60f...   \n3  https://images.indianexpress.com/2023/10/edit-...   \n4  https://static.timesofisrael.com/www/uploads/2...   \n\n                 published_at  \\\n0  2023-10-30 10:12:35.000000   \n1  2023-10-06 04:40:02.000000   \n2  2023-10-30 10:53:30.000000   \n3  2023-10-06 01:20:24.000000   \n4  2023-10-27 01:08:34.000000   \n\n                                             content category  \\\n0  UN Secretary-General Antonio Guterres urged th...    Nepal   \n1  RANDEBOO2023718()WEB2023 Autumn Winter \\n\"Nepa...    Nepal   \n2  Kathmandu, Nepal  UN Secretary-General Antonio...    Nepal   \n3  At least 14 persons lost their lives and more ...    Nepal   \n4  Scores of foreign citizens were killed, taken ...    Nepal   \n\n                                        full_content  \n0  UN Secretary-General Antonio Guterres urged th...  \n1                                                NaN  \n2                                                NaN  \n3  At least 14 persons lost their lives and more ...  \n4                                                NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>article_id</th>\n      <th>source_id</th>\n      <th>source_name</th>\n      <th>author</th>\n      <th>title</th>\n      <th>description</th>\n      <th>url</th>\n      <th>url_to_image</th>\n      <th>published_at</th>\n      <th>content</th>\n      <th>category</th>\n      <th>full_content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>89541</td>\n      <td>NaN</td>\n      <td>International Business Times</td>\n      <td>Paavan MATHEMA</td>\n      <td>UN Chief Urges World To 'Stop The Madness' Of ...</td>\n      <td>UN Secretary-General Antonio Guterres urged th...</td>\n      <td>https://www.ibtimes.com/un-chief-urges-world-s...</td>\n      <td>https://d.ibtimes.com/en/full/4496078/nepals-g...</td>\n      <td>2023-10-30 10:12:35.000000</td>\n      <td>UN Secretary-General Antonio Guterres urged th...</td>\n      <td>Nepal</td>\n      <td>UN Secretary-General Antonio Guterres urged th...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>89542</td>\n      <td>NaN</td>\n      <td>Prtimes.jp</td>\n      <td>NaN</td>\n      <td>RANDEBOOよりワンランク上の大人っぽさが漂うニットとベストが新登場。</td>\n      <td>[株式会社Ainer]\\nRANDEBOO（ランデブー）では2023年7月18日(火)より公...</td>\n      <td>https://prtimes.jp/main/html/rd/p/000000147.00...</td>\n      <td>https://prtimes.jp/i/32220/147/ogp/d32220-147-...</td>\n      <td>2023-10-06 04:40:02.000000</td>\n      <td>RANDEBOO2023718()WEB2023 Autumn Winter \\n\"Nepa...</td>\n      <td>Nepal</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>89543</td>\n      <td>NaN</td>\n      <td>VOA News</td>\n      <td>webdesk@voanews.com (Agence France-Presse)</td>\n      <td>UN Chief Urges World to 'Stop the Madness' of ...</td>\n      <td>UN Secretary-General Antonio Guterres urged th...</td>\n      <td>https://www.voanews.com/a/un-chief-urges-world...</td>\n      <td>https://gdb.voanews.com/01000000-0a00-0242-60f...</td>\n      <td>2023-10-30 10:53:30.000000</td>\n      <td>Kathmandu, Nepal  UN Secretary-General Antonio...</td>\n      <td>Nepal</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>89545</td>\n      <td>NaN</td>\n      <td>The Indian Express</td>\n      <td>Editorial</td>\n      <td>Sikkim warning: Hydroelectricity push must be ...</td>\n      <td>Ecologists caution against the adverse effects...</td>\n      <td>https://indianexpress.com/article/opinion/edit...</td>\n      <td>https://images.indianexpress.com/2023/10/edit-...</td>\n      <td>2023-10-06 01:20:24.000000</td>\n      <td>At least 14 persons lost their lives and more ...</td>\n      <td>Nepal</td>\n      <td>At least 14 persons lost their lives and more ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>89547</td>\n      <td>NaN</td>\n      <td>The Times of Israel</td>\n      <td>Jacob Magid</td>\n      <td>200 foreigners, dual nationals cut down in Ham...</td>\n      <td>France lost 35 citizens, Thailand 33, US 31, U...</td>\n      <td>https://www.timesofisrael.com/200-foreigners-d...</td>\n      <td>https://static.timesofisrael.com/www/uploads/2...</td>\n      <td>2023-10-27 01:08:34.000000</td>\n      <td>Scores of foreign citizens were killed, taken ...</td>\n      <td>Nepal</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bs1lhMGJOWSr"
   },
   "source": [
    "# **Data preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3hVEzuAdmIWZ"
   },
   "outputs": [],
   "source": [
    "# Function to preprocess source_name into source_id\n",
    "def preprocess_source_id(source_name):\n",
    "    # Convert to lowercase\n",
    "    source_id = source_name.lower()\n",
    "    # Replace spaces with hyphens\n",
    "    source_id = source_id.replace(\" \", \"-\")\n",
    "    # Keep only alphanumeric characters, hyphens, brackets and full stops\n",
    "    source_id = re.sub(r'[^\\w\\-.()]+', '', source_id)\n",
    "    return source_id\n",
    "\n",
    "# Apply the function to the source_name column to create the source_id column\n",
    "news_data['source_id'] = news_data['source_name'].apply(preprocess_source_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "joe7r84GnkXa"
   },
   "outputs": [],
   "source": [
    "# Preprocessing for author column missing values\n",
    "news_data['author'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Deal with full_content\n",
    "news_data.dropna(subset=['full_content'], inplace=True)\n",
    "\n",
    "# Since url_to_image is not important we can fill with placeholder\n",
    "news_data['url_to_image'].fillna('http://example.com/placeholder.jpg', inplace=True)\n",
    "\n",
    "# same with description column, use placeholder, unless otherwise important\n",
    "news_data['description'].fillna('No description provided', inplace=True)\n",
    "\n",
    "# Replace missing values in 'category' with 'Unknown'\n",
    "news_data['category'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Replace missing values in 'title' with 'No title provided'\n",
    "news_data['title'].fillna('No title provided', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ulw2ymm0qT-4"
   },
   "outputs": [],
   "source": [
    "# Perform preprocessing and clean missing values on the domains_info dataset\n",
    "domains_location_data.dropna(subset=['Country'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_jAlpZm3rqr"
   },
   "source": [
    "### More Preprocessing, on date fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "NwIWpCawiG0S"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "time data '2023-10-30 10:12:35.000000' does not match format 'ISO8601' (match)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 13\u001B[0m\n\u001B[1;32m     10\u001B[0m news_data\u001B[38;5;241m.\u001B[39mdrop_duplicates(inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Convert the 'published_at' column to datetime format\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m news_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpublished_at\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mto_datetime(news_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpublished_at\u001B[39m\u001B[38;5;124m'\u001B[39m], \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mISO8601\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# news_data['published_at'] = pd.to_datetime(news_data['published_at'])\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# Extract the year from the 'published_at' column\u001B[39;00m\n\u001B[1;32m     17\u001B[0m news_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124myear\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m news_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpublished_at\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mdt\u001B[38;5;241m.\u001B[39myear\n",
      "File \u001B[0;32m~/anaconda3/envs/10academyw0/lib/python3.12/site-packages/pandas/core/tools/datetimes.py:1068\u001B[0m, in \u001B[0;36mto_datetime\u001B[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001B[0m\n\u001B[1;32m   1066\u001B[0m         result \u001B[38;5;241m=\u001B[39m arg\u001B[38;5;241m.\u001B[39mmap(cache_array)\n\u001B[1;32m   1067\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1068\u001B[0m         values \u001B[38;5;241m=\u001B[39m convert_listlike(arg\u001B[38;5;241m.\u001B[39m_values, \u001B[38;5;28mformat\u001B[39m)\n\u001B[1;32m   1069\u001B[0m         result \u001B[38;5;241m=\u001B[39m arg\u001B[38;5;241m.\u001B[39m_constructor(values, index\u001B[38;5;241m=\u001B[39marg\u001B[38;5;241m.\u001B[39mindex, name\u001B[38;5;241m=\u001B[39marg\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1070\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arg, (ABCDataFrame, abc\u001B[38;5;241m.\u001B[39mMutableMapping)):\n",
      "File \u001B[0;32m~/anaconda3/envs/10academyw0/lib/python3.12/site-packages/pandas/core/tools/datetimes.py:430\u001B[0m, in \u001B[0;36m_convert_listlike_datetimes\u001B[0;34m(arg, format, name, tz, unit, errors, infer_datetime_format, dayfirst, yearfirst, exact)\u001B[0m\n\u001B[1;32m    427\u001B[0m         \u001B[38;5;28mformat\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    429\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mformat\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 430\u001B[0m     res \u001B[38;5;241m=\u001B[39m _to_datetime_with_format(\n\u001B[1;32m    431\u001B[0m         arg, orig_arg, name, tz, \u001B[38;5;28mformat\u001B[39m, exact, errors, infer_datetime_format\n\u001B[1;32m    432\u001B[0m     )\n\u001B[1;32m    433\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m res \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    434\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
      "File \u001B[0;32m~/anaconda3/envs/10academyw0/lib/python3.12/site-packages/pandas/core/tools/datetimes.py:538\u001B[0m, in \u001B[0;36m_to_datetime_with_format\u001B[0;34m(arg, orig_arg, name, tz, fmt, exact, errors, infer_datetime_format)\u001B[0m\n\u001B[1;32m    535\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m _box_as_indexlike(result, utc\u001B[38;5;241m=\u001B[39mutc, name\u001B[38;5;241m=\u001B[39mname)\n\u001B[1;32m    537\u001B[0m \u001B[38;5;66;03m# fallback\u001B[39;00m\n\u001B[0;32m--> 538\u001B[0m res \u001B[38;5;241m=\u001B[39m _array_strptime_with_fallback(\n\u001B[1;32m    539\u001B[0m     arg, name, tz, fmt, exact, errors, infer_datetime_format\n\u001B[1;32m    540\u001B[0m )\n\u001B[1;32m    541\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
      "File \u001B[0;32m~/anaconda3/envs/10academyw0/lib/python3.12/site-packages/pandas/core/tools/datetimes.py:473\u001B[0m, in \u001B[0;36m_array_strptime_with_fallback\u001B[0;34m(arg, name, tz, fmt, exact, errors, infer_datetime_format)\u001B[0m\n\u001B[1;32m    470\u001B[0m utc \u001B[38;5;241m=\u001B[39m tz \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutc\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    472\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 473\u001B[0m     result, timezones \u001B[38;5;241m=\u001B[39m array_strptime(arg, fmt, exact\u001B[38;5;241m=\u001B[39mexact, errors\u001B[38;5;241m=\u001B[39merrors)\n\u001B[1;32m    474\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m OutOfBoundsDatetime:\n\u001B[1;32m    475\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m errors \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraise\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[0;32m~/anaconda3/envs/10academyw0/lib/python3.12/site-packages/pandas/_libs/tslibs/strptime.pyx:150\u001B[0m, in \u001B[0;36mpandas._libs.tslibs.strptime.array_strptime\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: time data '2023-10-30 10:12:35.000000' does not match format 'ISO8601' (match)"
     ]
    }
   ],
   "source": [
    "# prompt: Using dataframe news_data: perform data preprocessing and cleaning\n",
    "\n",
    "# Check for missing values\n",
    "news_data.isnull().sum()\n",
    "\n",
    "# Drop rows with missing values\n",
    "news_data.dropna(inplace=True)\n",
    "\n",
    "# Remove duplicate rows\n",
    "news_data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Convert the 'published_at' column to datetime format\n",
    "news_data['published_at'] = pd.to_datetime(news_data['published_at'], format='ISO8601')\n",
    "# news_data['published_at'] = pd.to_datetime(news_data['published_at'])\n",
    "\n",
    "# Extract the year from the 'published_at' column\n",
    "news_data['year'] = news_data['published_at'].dt.year\n",
    "\n",
    "# Group the data by year and calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 643
    },
    "id": "SlHyrPjRmTdm",
    "outputId": "fa2023b0-47c3-4516-d8c4-6dd9b0713e6f"
   },
   "outputs": [],
   "source": [
    "news_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CqYRkuRMPI0j",
    "outputId": "00b24761-79b1-4721-a9b3-5c0a98edd3a1"
   },
   "outputs": [],
   "source": [
    "missing_values_news_data = news_data.isnull().sum()\n",
    "print(missing_values_news_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x5UAEg61QeDm",
    "outputId": "4740c3fa-6487-4c61-fa05-db3793c49c85"
   },
   "outputs": [],
   "source": [
    "missing_values_domains_location_data = domains_location_data.isnull().sum()\n",
    "print(missing_values_domains_location_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f-23hlE5Qkir",
    "outputId": "eaf56a0d-1b5f-468a-84b7-c501ca98f578"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GlobalRank        0\n",
      "TldRank           0\n",
      "Domain            0\n",
      "TLD               0\n",
      "RefSubNets        0\n",
      "RefIPs            0\n",
      "IDN_Domain        0\n",
      "IDN_TLD           0\n",
      "PrevGlobalRank    0\n",
      "PrevTldRank       0\n",
      "PrevRefSubNets    0\n",
      "PrevRefIPs        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_values_traffic_data = traffic_data.isnull().sum()\n",
    "print(missing_values_traffic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TAERvg3tOFQi",
    "outputId": "5c307a90-8eeb-4ab2-c441-847133a30af3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SourceCommonName', 'location', 'Country'], dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domains_location_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpXwKG8EQ1nv"
   },
   "source": [
    "### Fix missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ajb41xNgOCXI",
    "outputId": "d3d6c08d-d8be-4f2f-ee39-c8a309c8acf8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['GlobalRank', 'TldRank', 'Domain', 'TLD', 'RefSubNets', 'RefIPs',\n",
       "       'IDN_Domain', 'IDN_TLD', 'PrevGlobalRank', 'PrevTldRank',\n",
       "       'PrevRefSubNets', 'PrevRefIPs'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6B-6yWtuQ1J8",
    "outputId": "24822425-09d8-464f-cf79-7f5a230fd447"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article_id      0\n",
      "source_id       0\n",
      "source_name     0\n",
      "author          0\n",
      "title           0\n",
      "description     0\n",
      "url             0\n",
      "url_to_image    0\n",
      "published_at    0\n",
      "content         0\n",
      "category        0\n",
      "full_content    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "news_data = news_data.dropna()\n",
    "missing_values_news_data = news_data.isnull().sum()\n",
    "print(missing_values_news_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMAjSrk-PnT6"
   },
   "source": [
    "### Check outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JabKCySD2Wxt",
    "outputId": "4d951df1-1de4-4a6e-a0e0-f4f1ffb9692d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article_id      False\n",
       "source_id       False\n",
       "source_name     False\n",
       "author          False\n",
       "title           False\n",
       "description     False\n",
       "url             False\n",
       "url_to_image    False\n",
       "published_at    False\n",
       "content         False\n",
       "category        False\n",
       "full_content    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "news_data.isnull().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zz_uLmTG2Au3",
    "outputId": "8ad4140c-f6a8-4378-dab3-e8455aa12673"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54889"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TEcWLyWp2i8Z",
    "outputId": "edc96835-1bf7-4980-87c5-b16cd1d37cde"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SourceCommonName    False\n",
       "location            False\n",
       "Country             False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domains_location_data.isnull().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H-gE94WN2oCm",
    "outputId": "35b448c8-5500-4149-c148-b4cca514aa2f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domains_location_data.empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yLRUkzgn9ro_",
    "outputId": "326130c5-767b-4544-f7ca-d1f49e825d84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['article_id', 'source_id', 'source_name', 'author', 'title',\n",
       "       'description', 'url', 'url_to_image', 'published_at', 'content',\n",
       "       'category', 'full_content'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cBw8UILY94f4",
    "outputId": "82d810cf-9438-421e-d479-4bee3db97a85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 54889 entries, 0 to 105374\n",
      "Data columns (total 12 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   article_id    54889 non-null  int64 \n",
      " 1   source_id     54889 non-null  object\n",
      " 2   source_name   54889 non-null  object\n",
      " 3   author        54889 non-null  object\n",
      " 4   title         54889 non-null  object\n",
      " 5   description   54889 non-null  object\n",
      " 6   url           54889 non-null  object\n",
      " 7   url_to_image  54889 non-null  object\n",
      " 8   published_at  54889 non-null  object\n",
      " 9   content       54889 non-null  object\n",
      " 10  category      54889 non-null  object\n",
      " 11  full_content  54889 non-null  object\n",
      "dtypes: int64(1), object(11)\n",
      "memory usage: 5.4+ MB\n"
     ]
    }
   ],
   "source": [
    "news_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "c4uoVetA99Aw",
    "outputId": "ef936e63-8559-482f-ff9a-30e1579a4270"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>54889.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>305940.568019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>220864.360813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>418.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>105821.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>271035.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>469868.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>781308.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          article_id\n",
       "count   54889.000000\n",
       "mean   305940.568019\n",
       "std    220864.360813\n",
       "min       418.000000\n",
       "25%    105821.000000\n",
       "50%    271035.000000\n",
       "75%    469868.000000\n",
       "max    781308.000000"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yb02ddpOWSgZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9g7hgzhqWcBZ"
   },
   "source": [
    "# **NLP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbf8QaIaWmKi"
   },
   "source": [
    "## Keyword Extraction with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "HY56te74WfzB"
   },
   "outputs": [],
   "source": [
    "def extract_keywords_custom_tfidf(text_data, max_features=10):\n",
    "    # Use the 'article' column as the text data\n",
    "    array_text = text_data.dropna().tolist()\n",
    "\n",
    "     # Extend the stop words used by CountVectorizer\n",
    "    stop_words = text.ENGLISH_STOP_WORDS.union(['said', 'new'])\n",
    "\n",
    "    # Convert the set of stop words to a list\n",
    "    stop_words = list(stop_words)\n",
    "\n",
    "    # Initialize the CountVectorizer with stop_words set to 'english'\n",
    "    vectorizer = CountVectorizer(max_features=max_features, stop_words=stop_words)\n",
    "    \n",
    "    # Fit the vectorizer on the text data\n",
    "    tf = vectorizer.fit_transform([x.lower() for x in array_text])\n",
    "    tf = tf.toarray()\n",
    "    tf = log(tf + 1)\n",
    "\n",
    "    # Compute IDF values\n",
    "    df = pd.DataFrame(tf, columns=vectorizer.get_feature_names_out())\n",
    "    # calculates the Inverse Document Frequency (IDF) for each word in the text data.\n",
    "    idf = (len(array_text) / (df > 0).sum()).apply(log)\n",
    "\n",
    "    # We are ready to multiply the TF and IDF values to get the TF-IDF values.\n",
    "    tfidf = tf.copy()\n",
    "    words = array(vectorizer.get_feature_names_out())\n",
    "\n",
    "    for word in words:\n",
    "        tfidf[:, words == word] = tfidf[:, words == word] * idf[word]\n",
    "\n",
    "    keywords = []\n",
    "    for j in range(tfidf.shape[0]):\n",
    "        # Get the top 5 words with the highest TF-IDF values\n",
    "        top_words = [words[i] for i in tfidf[j].argsort()[-5:][::-1]]\n",
    "        keywords.append(top_words)\n",
    "\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract keywords for the first 10 and last 10 articles' title and content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract keywords for the first 10 and last 10 articles' title and content\n",
    "limited_news_data = pd.concat([news_data.head(10), news_data.tail(10)])\n",
    "limited_news_data['title_keywords'] = extract_keywords_custom_tfidf(limited_news_data['title'])\n",
    "limited_news_data['content_keywords'] = extract_keywords_custom_tfidf(limited_news_data['full_content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the added columns of keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>source_name</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>url_to_image</th>\n",
       "      <th>published_at</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>full_content</th>\n",
       "      <th>title_keywords</th>\n",
       "      <th>content_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89541</td>\n",
       "      <td>international-business-times</td>\n",
       "      <td>International Business Times</td>\n",
       "      <td>Paavan MATHEMA</td>\n",
       "      <td>UN Chief Urges World To 'Stop The Madness' Of ...</td>\n",
       "      <td>UN Secretary-General Antonio Guterres urged th...</td>\n",
       "      <td>https://www.ibtimes.com/un-chief-urges-world-s...</td>\n",
       "      <td>https://d.ibtimes.com/en/full/4496078/nepals-g...</td>\n",
       "      <td>2023-10-30 10:12:35.000000</td>\n",
       "      <td>UN Secretary-General Antonio Guterres urged th...</td>\n",
       "      <td>Nepal</td>\n",
       "      <td>UN Secretary-General Antonio Guterres urged th...</td>\n",
       "      <td>[world, climate, chief, change, pay]</td>\n",
       "      <td>[world, people, million, state, israel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89545</td>\n",
       "      <td>the-indian-express</td>\n",
       "      <td>The Indian Express</td>\n",
       "      <td>Editorial</td>\n",
       "      <td>Sikkim warning: Hydroelectricity push must be ...</td>\n",
       "      <td>Ecologists caution against the adverse effects...</td>\n",
       "      <td>https://indianexpress.com/article/opinion/edit...</td>\n",
       "      <td>https://images.indianexpress.com/2023/10/edit-...</td>\n",
       "      <td>2023-10-06 01:20:24.000000</td>\n",
       "      <td>At least 14 persons lost their lives and more ...</td>\n",
       "      <td>Nepal</td>\n",
       "      <td>At least 14 persons lost their lives and more ...</td>\n",
       "      <td>[world, pay, party, palestine, india]</td>\n",
       "      <td>[state, indian, government, people, world]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>89551</td>\n",
       "      <td>al-jazeera-english</td>\n",
       "      <td>Al Jazeera English</td>\n",
       "      <td>Kaushik Raj</td>\n",
       "      <td>Pro-Israel rallies allowed in India but Palest...</td>\n",
       "      <td>India, the first non-Arab country to recognise...</td>\n",
       "      <td>https://www.aljazeera.com/news/2023/10/25/pro-...</td>\n",
       "      <td>https://www.aljazeera.com/wp-content/uploads/2...</td>\n",
       "      <td>2023-10-25 09:58:17.000000</td>\n",
       "      <td>New Delhi, India Israels relentless bombing of...</td>\n",
       "      <td>Nepal</td>\n",
       "      <td>India, the first non-Arab country to recognise...</td>\n",
       "      <td>[palestine, india, world, pay, party]</td>\n",
       "      <td>[israel, india, state, government, indian]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>89555</td>\n",
       "      <td>the-indian-express</td>\n",
       "      <td>The Indian Express</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>No nation in the world is buying more planes t...</td>\n",
       "      <td>India's largest airlines have ordered nearly 1...</td>\n",
       "      <td>https://indianexpress.com/article/business/avi...</td>\n",
       "      <td>https://images.indianexpress.com/2023/11/igiai...</td>\n",
       "      <td>2023-11-02 05:48:58.000000</td>\n",
       "      <td>No nation in the world is buying as many airpl...</td>\n",
       "      <td>Nepal</td>\n",
       "      <td>Written by Alex Travelli and Hari Kumar No nat...</td>\n",
       "      <td>[world, india, pay, party, palestine]</td>\n",
       "      <td>[india, million, indian, 000, world]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>89563</td>\n",
       "      <td>the-times-of-india</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>Durgesh Nandan Jha</td>\n",
       "      <td>PM Hasina’s war on terror gets daughter India’...</td>\n",
       "      <td>India News: NEW DELHI: India preferred Banglad...</td>\n",
       "      <td>https://timesofindia.indiatimes.com/india/pm-h...</td>\n",
       "      <td>https://static.toiimg.com/thumb/msid-47529300,...</td>\n",
       "      <td>2023-11-02 01:12:47.000000</td>\n",
       "      <td>Ranked! Worlds most loved landmarks; Taj Mahal...</td>\n",
       "      <td>Nepal</td>\n",
       "      <td>NEW DELHI: India preferred Bangladesh over Nep...</td>\n",
       "      <td>[india, world, pay, party, palestine]</td>\n",
       "      <td>[india, world, state, people, million]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    article_id                     source_id                   source_name  \\\n",
       "0        89541  international-business-times  International Business Times   \n",
       "3        89545            the-indian-express            The Indian Express   \n",
       "6        89551            al-jazeera-english            Al Jazeera English   \n",
       "7        89555            the-indian-express            The Indian Express   \n",
       "12       89563            the-times-of-india            The Times of India   \n",
       "\n",
       "                author                                              title  \\\n",
       "0       Paavan MATHEMA  UN Chief Urges World To 'Stop The Madness' Of ...   \n",
       "3            Editorial  Sikkim warning: Hydroelectricity push must be ...   \n",
       "6          Kaushik Raj  Pro-Israel rallies allowed in India but Palest...   \n",
       "7       New York Times  No nation in the world is buying more planes t...   \n",
       "12  Durgesh Nandan Jha  PM Hasina’s war on terror gets daughter India’...   \n",
       "\n",
       "                                          description  \\\n",
       "0   UN Secretary-General Antonio Guterres urged th...   \n",
       "3   Ecologists caution against the adverse effects...   \n",
       "6   India, the first non-Arab country to recognise...   \n",
       "7   India's largest airlines have ordered nearly 1...   \n",
       "12  India News: NEW DELHI: India preferred Banglad...   \n",
       "\n",
       "                                                  url  \\\n",
       "0   https://www.ibtimes.com/un-chief-urges-world-s...   \n",
       "3   https://indianexpress.com/article/opinion/edit...   \n",
       "6   https://www.aljazeera.com/news/2023/10/25/pro-...   \n",
       "7   https://indianexpress.com/article/business/avi...   \n",
       "12  https://timesofindia.indiatimes.com/india/pm-h...   \n",
       "\n",
       "                                         url_to_image  \\\n",
       "0   https://d.ibtimes.com/en/full/4496078/nepals-g...   \n",
       "3   https://images.indianexpress.com/2023/10/edit-...   \n",
       "6   https://www.aljazeera.com/wp-content/uploads/2...   \n",
       "7   https://images.indianexpress.com/2023/11/igiai...   \n",
       "12  https://static.toiimg.com/thumb/msid-47529300,...   \n",
       "\n",
       "                  published_at  \\\n",
       "0   2023-10-30 10:12:35.000000   \n",
       "3   2023-10-06 01:20:24.000000   \n",
       "6   2023-10-25 09:58:17.000000   \n",
       "7   2023-11-02 05:48:58.000000   \n",
       "12  2023-11-02 01:12:47.000000   \n",
       "\n",
       "                                              content category  \\\n",
       "0   UN Secretary-General Antonio Guterres urged th...    Nepal   \n",
       "3   At least 14 persons lost their lives and more ...    Nepal   \n",
       "6   New Delhi, India Israels relentless bombing of...    Nepal   \n",
       "7   No nation in the world is buying as many airpl...    Nepal   \n",
       "12  Ranked! Worlds most loved landmarks; Taj Mahal...    Nepal   \n",
       "\n",
       "                                         full_content  \\\n",
       "0   UN Secretary-General Antonio Guterres urged th...   \n",
       "3   At least 14 persons lost their lives and more ...   \n",
       "6   India, the first non-Arab country to recognise...   \n",
       "7   Written by Alex Travelli and Hari Kumar No nat...   \n",
       "12  NEW DELHI: India preferred Bangladesh over Nep...   \n",
       "\n",
       "                           title_keywords  \\\n",
       "0    [world, climate, chief, change, pay]   \n",
       "3   [world, pay, party, palestine, india]   \n",
       "6   [palestine, india, world, pay, party]   \n",
       "7   [world, india, pay, party, palestine]   \n",
       "12  [india, world, pay, party, palestine]   \n",
       "\n",
       "                              content_keywords  \n",
       "0      [world, people, million, state, israel]  \n",
       "3   [state, indian, government, people, world]  \n",
       "6   [israel, india, state, government, indian]  \n",
       "7         [india, million, indian, 000, world]  \n",
       "12      [india, world, state, people, million]  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limited_news_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity of keywords between title and keywords in the news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220
    },
    "id": "quw05pg6Wp4K",
    "outputId": "6e9197fb-78c0-46c2-8666-35b728804646"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity score between keywords in the title and content of the articles is as follows:\n",
      "article 1: 0.18886369489717672\n",
      "article 2: 0.41171810764997363\n",
      "article 3: 0.41171810764997363\n",
      "article 4: 0.41171810764997363\n",
      "article 5: 0.41171810764997363\n",
      "article 6: 0.22109649138855314\n",
      "article 7: 0.41171810764997363\n",
      "article 8: 0.41171810764997363\n",
      "article 9: 0.25158568922967073\n",
      "article 10: 0.41171810764997363\n",
      "article 11: 0.2761797570583058\n",
      "article 12: 0.41171810764997363\n",
      "article 13: 0.41171810764997363\n",
      "article 14: 0.41171810764997363\n",
      "article 15: 0.41171810764997363\n",
      "article 16: 0.41171810764997363\n",
      "article 17: 0.41171810764997363\n",
      "article 18: 0.41171810764997363\n",
      "article 19: 0.2526691720356441\n",
      "article 20: 0.41171810764997363\n"
     ]
    }
   ],
   "source": [
    "# Convert the list of keywords for each article into a string\n",
    "limited_news_data['title_keywords_str'] = limited_news_data['title_keywords'].apply(' '.join)\n",
    "limited_news_data['content_keywords_str'] = limited_news_data['content_keywords'].apply(' '.join)\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Convert the keywords into TF-IDF vectors\n",
    "title_tfidf = vectorizer.fit_transform(limited_news_data['title_keywords_str'])\n",
    "content_tfidf = vectorizer.transform(limited_news_data['content_keywords_str'])\n",
    "\n",
    "# Calculate the cosine similarity between the title and content keywords for each article\n",
    "similarity_scores = cosine_similarity(title_tfidf, content_tfidf)\n",
    "\n",
    "# Print the similarity scores\n",
    "print(\"The similarity score between keywords in the title and content of the articles is as follows:\")\n",
    "for i, score in enumerate(similarity_scores):\n",
    "    print(f\"article {i+1}: {score[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-087zxdCXDE8"
   },
   "source": [
    "## Categorize the title/content into known set of topic categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the path of the cache directory\n",
    "cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"huggingface\", \"models\")\n",
    "\n",
    "# Define the name of the model directory to delete\n",
    "model_dir = \"sentence-transformers_all-MiniLM-L6-v2\"\n",
    "\n",
    "# Define the full path of the model directory\n",
    "full_model_dir = os.path.join(cache_dir, model_dir)\n",
    "\n",
    "# Delete the model directory\n",
    "shutil.rmtree(full_model_dir, ignore_errors=True)\n",
    "\n",
    "# Now, you can re-run your BERTopic code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/hilla/anaconda3/envs/10academyw0/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_17057/3998661163.py\", line 11, in <module>\n",
      "    topics, probs = topic_model.fit_transform(docs)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hilla/anaconda3/envs/10academyw0/lib/python3.12/site-packages/bertopic/_bertopic.py\", line 433, in fit_transform\n",
      "    self._extract_topics(documents, embeddings=embeddings, verbose=self.verbose)\n",
      "  File \"/home/hilla/anaconda3/envs/10academyw0/lib/python3.12/site-packages/bertopic/_bertopic.py\", line 3636, in _extract_topics\n",
      "    self.c_tf_idf_, words = self._c_tf_idf(documents_per_topic)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hilla/anaconda3/envs/10academyw0/lib/python3.12/site-packages/bertopic/_bertopic.py\", line 3835, in _c_tf_idf\n",
      "    self.vectorizer_model.fit(documents)\n",
      "  File \"/home/hilla/anaconda3/envs/10academyw0/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 1340, in fit\n",
      "    self.fit_transform(raw_documents)\n",
      "  File \"/home/hilla/anaconda3/envs/10academyw0/lib/python3.12/site-packages/sklearn/base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hilla/anaconda3/envs/10academyw0/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 1389, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hilla/anaconda3/envs/10academyw0/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line None, in _count_vocab\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hilla/anaconda3/envs/10academyw0/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hilla/anaconda3/envs/10academyw0/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hilla/anaconda3/envs/10academyw0/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hilla/anaconda3/envs/10academyw0/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hilla/anaconda3/envs/10academyw0/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hilla/anaconda3/envs/10academyw0/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hilla/anaconda3/envs/10academyw0/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/hilla/anaconda3/envs/10academyw0/lib/python3.12/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/home/hilla/anaconda3/envs/10academyw0/lib/python3.12/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hilla/anaconda3/envs/10academyw0/lib/python3.12/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/home/hilla/anaconda3/envs/10academyw0/lib/python3.12/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hilla/anaconda3/envs/10academyw0/lib/python3.12/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/home/hilla/anaconda3/envs/10academyw0/lib/python3.12/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"/home/hilla/anaconda3/envs/10academyw0/lib/python3.12/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze topics and trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bertopic'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[127], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mseaborn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01msns\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mbertopic\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BERTopic\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m fetch_20newsgroups\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcollections\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Counter\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'bertopic'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bertopic import BERTopic\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from collections import Counter\n",
    "\n",
    "# Load the data\n",
    "docs = news_data['content'].values.tolist()\n",
    "\n",
    "# Create an instance of BERTopic\n",
    "topic_model = BERTopic()\n",
    "\n",
    "# Fit BERTopic to the news document and transform the news documents into topics\n",
    "topics, _ = topic_model.fit_transform(docs)\n",
    "\n",
    "# Add topics to the original dataframe\n",
    "news_data['topic'] = topics\n",
    "\n",
    "# Analyze the topics and trends\n",
    "# Determine which websites reported the most diverse topics\n",
    "website_topic_counts = news_data.groupby('source_name')['topic'].nunique().sort_values(ascending=False)\n",
    "print(\"Websites with the most diverse topics:\")\n",
    "print(website_topic_counts)\n",
    "\n",
    "# Plot a 2D scatter plot to visualize the trends\n",
    "# Convert date to datetime format and extract date only, assuming 'date' is your date column\n",
    "news_data['date'] = pd.to_datetime(news_data['date']).dt.date\n",
    "\n",
    "# Count the number of each topic on each date\n",
    "topic_counts = news_data.groupby(['date', 'topic']).size().reset_index(name='count')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=topic_counts, x='date', y='topic', size='count', legend=False, sizes=(20, 2000))\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Topic')\n",
    "plt.title('Topic Trends Over Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model the event that the news articles are written about"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model the event that the articles are covering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 10\u001B[0m\n\u001B[1;32m      7\u001B[0m nlp \u001B[38;5;241m=\u001B[39m spacy\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124men_core_web_sm\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Preprocess the data\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m news_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m news_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([token\u001B[38;5;241m.\u001B[39mlemma_ \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m nlp(x) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m token\u001B[38;5;241m.\u001B[39mis_stop \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m token\u001B[38;5;241m.\u001B[39mis_punct \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m token\u001B[38;5;241m.\u001B[39mlike_num]))\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Extract features from the text\u001B[39;00m\n\u001B[1;32m     13\u001B[0m vectorizer \u001B[38;5;241m=\u001B[39m TfidfVectorizer()\n",
      "File \u001B[0;32m~/anaconda3/envs/10academyw0/lib/python3.12/site-packages/pandas/core/series.py:4771\u001B[0m, in \u001B[0;36mSeries.apply\u001B[0;34m(self, func, convert_dtype, args, **kwargs)\u001B[0m\n\u001B[1;32m   4661\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[1;32m   4662\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   4663\u001B[0m     func: AggFuncType,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4666\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   4667\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[1;32m   4668\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   4669\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[1;32m   4670\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4769\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[1;32m   4770\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 4771\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m SeriesApply(\u001B[38;5;28mself\u001B[39m, func, convert_dtype, args, kwargs)\u001B[38;5;241m.\u001B[39mapply()\n",
      "File \u001B[0;32m~/anaconda3/envs/10academyw0/lib/python3.12/site-packages/pandas/core/apply.py:1123\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1120\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_str()\n\u001B[1;32m   1122\u001B[0m \u001B[38;5;66;03m# self.f is Callable\u001B[39;00m\n\u001B[0;32m-> 1123\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_standard()\n",
      "File \u001B[0;32m~/anaconda3/envs/10academyw0/lib/python3.12/site-packages/pandas/core/apply.py:1174\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1172\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1173\u001B[0m         values \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m)\u001B[38;5;241m.\u001B[39m_values\n\u001B[0;32m-> 1174\u001B[0m         mapped \u001B[38;5;241m=\u001B[39m lib\u001B[38;5;241m.\u001B[39mmap_infer(\n\u001B[1;32m   1175\u001B[0m             values,\n\u001B[1;32m   1176\u001B[0m             f,\n\u001B[1;32m   1177\u001B[0m             convert\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconvert_dtype,\n\u001B[1;32m   1178\u001B[0m         )\n\u001B[1;32m   1180\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[1;32m   1181\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[1;32m   1182\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[1;32m   1183\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[0;32m~/anaconda3/envs/10academyw0/lib/python3.12/site-packages/pandas/_libs/lib.pyx:2924\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[0;34m()\u001B[0m\n",
      "Cell \u001B[0;32mIn[13], line 10\u001B[0m, in \u001B[0;36m<lambda>\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m      7\u001B[0m nlp \u001B[38;5;241m=\u001B[39m spacy\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124men_core_web_sm\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Preprocess the data\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m news_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m news_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([token\u001B[38;5;241m.\u001B[39mlemma_ \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m nlp(x) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m token\u001B[38;5;241m.\u001B[39mis_stop \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m token\u001B[38;5;241m.\u001B[39mis_punct \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m token\u001B[38;5;241m.\u001B[39mlike_num]))\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Extract features from the text\u001B[39;00m\n\u001B[1;32m     13\u001B[0m vectorizer \u001B[38;5;241m=\u001B[39m TfidfVectorizer()\n",
      "File \u001B[0;32m~/anaconda3/envs/10academyw0/lib/python3.12/site-packages/spacy/language.py:1049\u001B[0m, in \u001B[0;36mLanguage.__call__\u001B[0;34m(self, text, disable, component_cfg)\u001B[0m\n\u001B[1;32m   1047\u001B[0m     error_handler \u001B[38;5;241m=\u001B[39m proc\u001B[38;5;241m.\u001B[39mget_error_handler()\n\u001B[1;32m   1048\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1049\u001B[0m     doc \u001B[38;5;241m=\u001B[39m proc(doc, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcomponent_cfg\u001B[38;5;241m.\u001B[39mget(name, {}))  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m   1050\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1051\u001B[0m     \u001B[38;5;66;03m# This typically happens if a component is not initialized\u001B[39;00m\n\u001B[1;32m   1052\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE109\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname)) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/10academyw0/lib/python3.12/site-packages/spacy/pipeline/trainable_pipe.pyx:52\u001B[0m, in \u001B[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/envs/10academyw0/lib/python3.12/site-packages/spacy/pipeline/tok2vec.py:126\u001B[0m, in \u001B[0;36mTok2Vec.predict\u001B[0;34m(self, docs)\u001B[0m\n\u001B[1;32m    124\u001B[0m     width \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mget_dim(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnO\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39malloc((\u001B[38;5;241m0\u001B[39m, width)) \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m docs]\n\u001B[0;32m--> 126\u001B[0m tokvecs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mpredict(docs)\n\u001B[1;32m    127\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tokvecs\n",
      "File \u001B[0;32m~/anaconda3/envs/10academyw0/lib/python3.12/site-packages/thinc/model.py:334\u001B[0m, in \u001B[0;36mModel.predict\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    330\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m OutT:\n\u001B[1;32m    331\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001B[39;00m\n\u001B[1;32m    332\u001B[0m \u001B[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001B[39;00m\n\u001B[1;32m    333\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 334\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_func(\u001B[38;5;28mself\u001B[39m, X, is_train\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/anaconda3/envs/10academyw0/lib/python3.12/site-packages/thinc/layers/chain.py:54\u001B[0m, in \u001B[0;36mforward\u001B[0;34m(model, X, is_train)\u001B[0m\n\u001B[1;32m     52\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m model\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[0;32m---> 54\u001B[0m     Y, inc_layer_grad \u001B[38;5;241m=\u001B[39m layer(X, is_train\u001B[38;5;241m=\u001B[39mis_train)\n\u001B[1;32m     55\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mappend(inc_layer_grad)\n\u001B[1;32m     56\u001B[0m     X \u001B[38;5;241m=\u001B[39m Y\n",
      "File \u001B[0;32m~/anaconda3/envs/10academyw0/lib/python3.12/site-packages/thinc/model.py:310\u001B[0m, in \u001B[0;36mModel.__call__\u001B[0;34m(self, X, is_train)\u001B[0m\n\u001B[1;32m    307\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT, is_train: \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[OutT, Callable]:\n\u001B[1;32m    308\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001B[39;00m\n\u001B[1;32m    309\u001B[0m \u001B[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 310\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_func(\u001B[38;5;28mself\u001B[39m, X, is_train\u001B[38;5;241m=\u001B[39mis_train)\n",
      "File \u001B[0;32m~/anaconda3/envs/10academyw0/lib/python3.12/site-packages/thinc/layers/with_array.py:42\u001B[0m, in \u001B[0;36mforward\u001B[0;34m(model, Xseq, is_train)\u001B[0m\n\u001B[1;32m     40\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\u001B[38;5;241m.\u001B[39mlayers[\u001B[38;5;241m0\u001B[39m](Xseq, is_train)\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 42\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(Tuple[SeqT, Callable], _list_forward(model, Xseq, is_train))\n",
      "File \u001B[0;32m~/anaconda3/envs/10academyw0/lib/python3.12/site-packages/thinc/layers/with_array.py:77\u001B[0m, in \u001B[0;36m_list_forward\u001B[0;34m(model, Xs, is_train)\u001B[0m\n\u001B[1;32m     75\u001B[0m lengths \u001B[38;5;241m=\u001B[39m NUMPY_OPS\u001B[38;5;241m.\u001B[39masarray1i([\u001B[38;5;28mlen\u001B[39m(seq) \u001B[38;5;28;01mfor\u001B[39;00m seq \u001B[38;5;129;01min\u001B[39;00m Xs])\n\u001B[1;32m     76\u001B[0m Xf \u001B[38;5;241m=\u001B[39m layer\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mflatten(Xs, pad\u001B[38;5;241m=\u001B[39mpad)\n\u001B[0;32m---> 77\u001B[0m Yf, get_dXf \u001B[38;5;241m=\u001B[39m layer(Xf, is_train)\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbackprop\u001B[39m(dYs: ListXd) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ListXd:\n\u001B[1;32m     80\u001B[0m     dYf \u001B[38;5;241m=\u001B[39m layer\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mflatten(dYs, pad\u001B[38;5;241m=\u001B[39mpad)\n",
      "File \u001B[0;32m~/anaconda3/envs/10academyw0/lib/python3.12/site-packages/thinc/model.py:310\u001B[0m, in \u001B[0;36mModel.__call__\u001B[0;34m(self, X, is_train)\u001B[0m\n\u001B[1;32m    307\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT, is_train: \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[OutT, Callable]:\n\u001B[1;32m    308\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001B[39;00m\n\u001B[1;32m    309\u001B[0m \u001B[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 310\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_func(\u001B[38;5;28mself\u001B[39m, X, is_train\u001B[38;5;241m=\u001B[39mis_train)\n",
      "File \u001B[0;32m~/anaconda3/envs/10academyw0/lib/python3.12/site-packages/thinc/layers/chain.py:54\u001B[0m, in \u001B[0;36mforward\u001B[0;34m(model, X, is_train)\u001B[0m\n\u001B[1;32m     52\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m model\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[0;32m---> 54\u001B[0m     Y, inc_layer_grad \u001B[38;5;241m=\u001B[39m layer(X, is_train\u001B[38;5;241m=\u001B[39mis_train)\n\u001B[1;32m     55\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mappend(inc_layer_grad)\n\u001B[1;32m     56\u001B[0m     X \u001B[38;5;241m=\u001B[39m Y\n",
      "File \u001B[0;32m~/anaconda3/envs/10academyw0/lib/python3.12/site-packages/thinc/model.py:310\u001B[0m, in \u001B[0;36mModel.__call__\u001B[0;34m(self, X, is_train)\u001B[0m\n\u001B[1;32m    307\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT, is_train: \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[OutT, Callable]:\n\u001B[1;32m    308\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001B[39;00m\n\u001B[1;32m    309\u001B[0m \u001B[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 310\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_func(\u001B[38;5;28mself\u001B[39m, X, is_train\u001B[38;5;241m=\u001B[39mis_train)\n",
      "File \u001B[0;32m~/anaconda3/envs/10academyw0/lib/python3.12/site-packages/thinc/layers/residual.py:41\u001B[0m, in \u001B[0;36mforward\u001B[0;34m(model, X, is_train)\u001B[0m\n\u001B[1;32m     38\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     39\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m d_output \u001B[38;5;241m+\u001B[39m dX\n\u001B[0;32m---> 41\u001B[0m Y, backprop_layer \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mlayers[\u001B[38;5;241m0\u001B[39m](X, is_train)\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(X, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m     43\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [X[i] \u001B[38;5;241m+\u001B[39m Y[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(X))], backprop\n",
      "File \u001B[0;32m~/anaconda3/envs/10academyw0/lib/python3.12/site-packages/thinc/model.py:310\u001B[0m, in \u001B[0;36mModel.__call__\u001B[0;34m(self, X, is_train)\u001B[0m\n\u001B[1;32m    307\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT, is_train: \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[OutT, Callable]:\n\u001B[1;32m    308\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001B[39;00m\n\u001B[1;32m    309\u001B[0m \u001B[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 310\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_func(\u001B[38;5;28mself\u001B[39m, X, is_train\u001B[38;5;241m=\u001B[39mis_train)\n",
      "File \u001B[0;32m~/anaconda3/envs/10academyw0/lib/python3.12/site-packages/thinc/layers/chain.py:54\u001B[0m, in \u001B[0;36mforward\u001B[0;34m(model, X, is_train)\u001B[0m\n\u001B[1;32m     52\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m model\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[0;32m---> 54\u001B[0m     Y, inc_layer_grad \u001B[38;5;241m=\u001B[39m layer(X, is_train\u001B[38;5;241m=\u001B[39mis_train)\n\u001B[1;32m     55\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mappend(inc_layer_grad)\n\u001B[1;32m     56\u001B[0m     X \u001B[38;5;241m=\u001B[39m Y\n",
      "File \u001B[0;32m~/anaconda3/envs/10academyw0/lib/python3.12/site-packages/thinc/model.py:310\u001B[0m, in \u001B[0;36mModel.__call__\u001B[0;34m(self, X, is_train)\u001B[0m\n\u001B[1;32m    307\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT, is_train: \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[OutT, Callable]:\n\u001B[1;32m    308\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001B[39;00m\n\u001B[1;32m    309\u001B[0m \u001B[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 310\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_func(\u001B[38;5;28mself\u001B[39m, X, is_train\u001B[38;5;241m=\u001B[39mis_train)\n",
      "File \u001B[0;32m~/anaconda3/envs/10academyw0/lib/python3.12/site-packages/thinc/layers/chain.py:54\u001B[0m, in \u001B[0;36mforward\u001B[0;34m(model, X, is_train)\u001B[0m\n\u001B[1;32m     52\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m model\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[0;32m---> 54\u001B[0m     Y, inc_layer_grad \u001B[38;5;241m=\u001B[39m layer(X, is_train\u001B[38;5;241m=\u001B[39mis_train)\n\u001B[1;32m     55\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mappend(inc_layer_grad)\n\u001B[1;32m     56\u001B[0m     X \u001B[38;5;241m=\u001B[39m Y\n",
      "    \u001B[0;31m[... skipping similar frames: Model.__call__ at line 310 (1 times)]\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/envs/10academyw0/lib/python3.12/site-packages/thinc/layers/chain.py:54\u001B[0m, in \u001B[0;36mforward\u001B[0;34m(model, X, is_train)\u001B[0m\n\u001B[1;32m     52\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m model\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[0;32m---> 54\u001B[0m     Y, inc_layer_grad \u001B[38;5;241m=\u001B[39m layer(X, is_train\u001B[38;5;241m=\u001B[39mis_train)\n\u001B[1;32m     55\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mappend(inc_layer_grad)\n\u001B[1;32m     56\u001B[0m     X \u001B[38;5;241m=\u001B[39m Y\n",
      "File \u001B[0;32m~/anaconda3/envs/10academyw0/lib/python3.12/site-packages/thinc/model.py:310\u001B[0m, in \u001B[0;36mModel.__call__\u001B[0;34m(self, X, is_train)\u001B[0m\n\u001B[1;32m    307\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT, is_train: \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[OutT, Callable]:\n\u001B[1;32m    308\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001B[39;00m\n\u001B[1;32m    309\u001B[0m \u001B[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 310\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_func(\u001B[38;5;28mself\u001B[39m, X, is_train\u001B[38;5;241m=\u001B[39mis_train)\n",
      "File \u001B[0;32m~/anaconda3/envs/10academyw0/lib/python3.12/site-packages/thinc/layers/maxout.py:52\u001B[0m, in \u001B[0;36mforward\u001B[0;34m(model, X, is_train)\u001B[0m\n\u001B[1;32m     50\u001B[0m W \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mget_param(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mW\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     51\u001B[0m W \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mreshape2f(W, nO \u001B[38;5;241m*\u001B[39m nP, nI)\n\u001B[0;32m---> 52\u001B[0m Y \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mgemm(X, W, trans2\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     53\u001B[0m Y \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mreshape1f(b, nO \u001B[38;5;241m*\u001B[39m nP)\n\u001B[1;32m     54\u001B[0m Z \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mreshape3f(Y, Y\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], nO, nP)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from collections import Counter\n",
    "import spacy\n",
    "\n",
    "# Load Spacy's English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Preprocess the data\n",
    "news_data['content'] = news_data['content'].apply(lambda x: ' '.join([token.lemma_ for token in nlp(x) if not token.is_stop and not token.is_punct and not token.like_num]))\n",
    "\n",
    "# Extract features from the text\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(news_data['content'])\n",
    "\n",
    "# Cluster the articles\n",
    "clustering = AgglomerativeClustering(n_clusters=500).fit(X.toarray())\n",
    "\n",
    "# Add the cluster labels to the dataframe\n",
    "news_data['cluster'] = clustering.labels_\n",
    "\n",
    "# Label the clusters\n",
    "cluster_labels = []\n",
    "for cluster in range(500):\n",
    "    # Get the articles in this cluster\n",
    "    articles = news_data[news_data['cluster'] == cluster]['content']\n",
    "\n",
    "    # Combine the articles into one long string\n",
    "    text = ' '.join(articles)\n",
    "\n",
    "    # Get the most common words in this text\n",
    "    words = Counter(text.split())\n",
    "    most_common_words = words.most_common(5)\n",
    "\n",
    "    # Use the most common words as the cluster label\n",
    "    label = ' '.join([word for word, _ in most_common_words])\n",
    "    cluster_labels.append(label)\n",
    "\n",
    "news_data['event'] = news_data['cluster'].apply(lambda x: cluster_labels[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "news_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering of news atricles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T16:44:58.228712Z",
     "start_time": "2024-04-10T16:44:55.919876Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'event'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/anaconda3/envs/10academyw0/lib/python3.12/site-packages/pandas/core/indexes/base.py:3802\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3801\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3802\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[1;32m   3803\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32m~/anaconda3/envs/10academyw0/lib/python3.12/site-packages/pandas/_libs/index.pyx:138\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/envs/10academyw0/lib/python3.12/site-packages/pandas/_libs/index.pyx:165\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'event'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# How many events are covered in the data?\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m num_events \u001B[38;5;241m=\u001B[39m news_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mevent\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mnunique()\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNumber of events: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_events\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Analyse the news sites that report events the earliest\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/10academyw0/lib/python3.12/site-packages/pandas/core/frame.py:3807\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3805\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   3806\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[0;32m-> 3807\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mget_loc(key)\n\u001B[1;32m   3808\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[1;32m   3809\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[0;32m~/anaconda3/envs/10academyw0/lib/python3.12/site-packages/pandas/core/indexes/base.py:3804\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3802\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[1;32m   3803\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m-> 3804\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3805\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3806\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3807\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3808\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[1;32m   3809\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'event'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# How many events are covered in the data?\n",
    "num_events = news_data['event'].nunique()\n",
    "print(f\"Number of events: {num_events}\")\n",
    "\n",
    "# Analyse the news sites that report events the earliest\n",
    "news_data['date'] = pd.to_datetime(news_data['date'])  # Ensure the date column is in datetime format\n",
    "earliest_reporting = news_data.sort_values('date').groupby('event').first()['source_name']\n",
    "print(\"News sites that reported each event the earliest:\")\n",
    "print(earliest_reporting)\n",
    "\n",
    "# Events with the highest reporting\n",
    "event_counts = news_data['event'].value_counts()\n",
    "print(\"Events with the highest reporting:\")\n",
    "print(event_counts)\n",
    "\n",
    "# The correlation between news sites reporting events?\n",
    "# Create a matrix where each cell represents the number of common events reported by a pair of news sites\n",
    "correlation_matrix = pd.crosstab(news_data['source_name'], news_data['event'])\n",
    "correlation_matrix = correlation_matrix.T.dot(correlation_matrix)\n",
    "print(\"Correlation matrix of news sites reporting events:\")\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Reduce the size of the dataset for testing purposes\n",
    "fraction = 0.02\n",
    "\n",
    "# Randomly sample the fraction of data\n",
    "news_data_sample = news_data.sample(frac=fraction)\n",
    "\n",
    "docs = news_data_sample['full_content'].values.tolist()\n",
    "\n",
    "# Import the necessary libraries\n",
    "from bertopic import BERTopic\n",
    "\n",
    "topic_model = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True)\n",
    "\n",
    "# Fit BERTopic to the news and transform the news into topics\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "# Get an overview of the topics\n",
    "topic_overview = topic_model.get_topic_info(), freq.head(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (10academyw0)",
   "language": "python",
   "name": "10academyw0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
